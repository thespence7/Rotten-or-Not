{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "newfolder = 'rottentomatoes'\n",
    "\n",
    "if not os.path.isdir(newfolder):\n",
    "    os.mkdir(newfolder)\n",
    "\n",
    "os.chdir(newfolder)\n",
    "\n",
    "website = \"https://www.rottentomatoes.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ascii_letters[:26].replace('x', '')  # remove x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(page, addition=''):\n",
    "    \"\"\"Fetches HTML data\"\"\"\n",
    "    headers = {'User-Agent': 'Opera/9.80 (X11; Linux i686; Ub'\n",
    "               'untu/14.10) Presto/2.12.388 Version/12.16'}\n",
    "    req = Request(page + addition, headers=headers)\n",
    "    open_request = urlopen(req).read()\n",
    "    soup = bs.BeautifulSoup(open_request, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critics_letters(letters):\n",
    "    \"\"\"Creates URL for 26 pages of critics, based on the first letter of their name\"\"\"\n",
    "    letters_url = list()\n",
    "    for elem in letters:\n",
    "        letters_url.append(\"/critics/authors?letter=\" + elem)\n",
    "    return letters_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critics_list(catalog):\n",
    "    \"\"\"Fetches the url of all listed critics\"\"\"\n",
    "    critics_url = list()\n",
    "    for ix, letter_pages in enumerate(catalog, 1):\n",
    "        for a in fetch(website, letter_pages).find_all(\"a\", {\"class\": \"a critic-authors__name\"}):\n",
    "            href_critic = a['href']\n",
    "            if str(href_critic)[:7] != \"/source\":\n",
    "                critics_url.append(href_critic + \"/movies\")\n",
    "        print('\\r1/4 — {:.2%} of movie critic URLs scraped.'.format(ix/len(catalog)), end='   ')\n",
    "        print('\\r{} pages of movie critic URLs successfully scraped.'.format(ix), end='  '); print()\n",
    "    return critics_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movies(catalog):\n",
    "    \"\"\"Fetches the url of the movies reviewed by the critic\"\"\"\n",
    "    movies_url = list()\n",
    "    errors = 0\n",
    "    for ix, critic_profile in enumerate(catalog, 1):\n",
    "        try:\n",
    "            checker = fetch(website, critic_profile).find_all(\"h2\", {\"class\": \"panel-heading js-review-type\"})\n",
    "            if len(checker) > 0:\n",
    "                if checker[0].text == \"Movie Reviews Only\":\n",
    "                    for td in fetch(website, critic_profile).find_all(\"td\",\n",
    "                                    {\"class\": \"col-xs-12 col-sm-6 critic-review-table__title-column\"}):\n",
    "                        for a in td.find_all(\"a\"):\n",
    "                            if a['href'] not in movies_url:\n",
    "                                movies_url.append(a['href'])\n",
    "        except:\n",
    "            errors += 1\n",
    "        print('\\r2/4 — {:.2%} of movie URLs scraped. Error rate: {:.2%}'.format(ix/len(catalog),\n",
    "                                        errors/ix), end='   ')\n",
    "    print('\\r{} movie URLs successfully scraped. Error rate: {:.2%}'.format(len(movies_url)-errors, errors/ix), end='\\n')\n",
    "    return movies_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_pages(catalog):\n",
    "    \"\"\"List the pages of reviews from all movies in chunks of 1000 and exports 17 csv files\"\"\"\n",
    "    review_pages_list = list()\n",
    "    errors = 0\n",
    "    for ix, movie in enumerate(catalog.iloc[:, 0], 1):\n",
    "        try:\n",
    "            soup_2 = fetch(movie, \"/reviews/?page=1\").find_all(\"span\", {\"class\", \"pageInfo\"})\n",
    "            if len(soup_2) >= 1:\n",
    "                for n in range(1, int(soup_2[0].text[-2:]) + 1):\n",
    "                    review_pages_list.append(movie + \"/reviews/?page=\" + str(n))\n",
    "        except:\n",
    "            errors += 1\n",
    "        print('\\r3/4 — {:.2%} of review page URLs scraped. Error rate: {:.2%}'.format(\n",
    "            ix/len(catalog), errors/ix), end='    ')\n",
    "    print('\\r{} review page URLs successfully scraped. Error rate: {:.2%}'.format(\n",
    "        len(review_pages_list)-errors, errors/ix), end='\\n')\n",
    "    return review_pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_review(catalog):\n",
    "    \"\"\"Scrapes all the reviews and rating from the pages\"\"\"\n",
    "    reviews = list()\n",
    "    errors = 0\n",
    "    for ix, page in enumerate(catalog.iloc[:, 0], 1):\n",
    "        try:\n",
    "            soup_2 = fetch(page, \"\").find_all(\"div\", {\"class\": \"col-xs-16 review_container\"})\n",
    "            for comment in soup_2:\n",
    "                comment_text = comment.find_all(\"div\", {\"class\": \"the_review\"})[0].text.strip()\n",
    "                icon = str(comment.find_all(\"div\")[0])\n",
    "                if \"fresh\" in icon:\n",
    "                    reviews.append('1 - ' + comment_text)\n",
    "                elif \"rotten\" in icon:\n",
    "                    reviews.append('0 - ' + comment_text)\n",
    "        except:\n",
    "            errors += 1\n",
    "        print('\\r4/4 — {:.2%} of reviews scraped. Error rate: {:.2%}'.format(ix/len(catalog),\n",
    "                                            errors/ix), end='    ')\n",
    "    print('\\r{} reviews successfully scraped. Error rate: {:.2%}'.format(\n",
    "        len(reviews)-errors, errors/ix), end='\\n')\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(document):\n",
    "    \"\"\"Prepares the document and exports it to the working directory\"\"\"\n",
    "    df = document\n",
    "    df['freshness'] = df.iloc[:, 0].str.split(' - ').str.get(0)\n",
    "    df['review'] = df.iloc[:, 0].str.split(' - ').str.get(1)\n",
    "    df = df.loc[df['review'].str.len() >= 18]\n",
    "    df = df.loc[:, ['freshness', 'review']]\n",
    "\n",
    "    num_to_keep = (df.shape[0] - df.freshness.astype(np.int32).sum()) // 10_000 * 10_000\n",
    "    rotten = df.loc[df.freshness == '0'].sample(num_to_keep)\n",
    "    fresh = df.loc[df.freshness == '1'].sample(num_to_keep)\n",
    "\n",
    "    df = pd.concat([rotten, fresh], axis=0, sort=False)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv('all_rotten_tomatoes_reviews.csv', index=False)\n",
    "\n",
    "    print('\\nThe web scraper has finished.',\n",
    "          '\\nCheck your directory: {}'.format(os.getcwd()),\n",
    "          '\\nThe file with all reviews is named: all_rotten_tomatoes_reviews.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pages of movie critic URLs successfully scraped.  \n",
      "2 pages of movie critic URLs successfully scraped.  \n",
      "3 pages of movie critic URLs successfully scraped.  \n",
      "4 pages of movie critic URLs successfully scraped.  \n",
      "5 pages of movie critic URLs successfully scraped.  \n",
      "6 pages of movie critic URLs successfully scraped.  \n",
      "7 pages of movie critic URLs successfully scraped.  \n",
      "8 pages of movie critic URLs successfully scraped.  \n",
      "9 pages of movie critic URLs successfully scraped.  \n",
      "10 pages of movie critic URLs successfully scraped.  \n",
      "11 pages of movie critic URLs successfully scraped.  \n",
      "12 pages of movie critic URLs successfully scraped.  \n",
      "13 pages of movie critic URLs successfully scraped.  \n",
      "14 pages of movie critic URLs successfully scraped.  \n",
      "15 pages of movie critic URLs successfully scraped.  \n",
      "16 pages of movie critic URLs successfully scraped.  \n",
      "17 pages of movie critic URLs successfully scraped.  \n",
      "18 pages of movie critic URLs successfully scraped.  \n",
      "19 pages of movie critic URLs successfully scraped.  \n",
      "20 pages of movie critic URLs successfully scraped.  \n",
      "21 pages of movie critic URLs successfully scraped.  \n",
      "22 pages of movie critic URLs successfully scraped.  \n",
      "23 pages of movie critic URLs successfully scraped.  \n",
      "24 pages of movie critic URLs successfully scraped.  \n",
      "25 pages of movie critic URLs successfully scraped.  \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ix' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1d46711ae9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcritic_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritics_letters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlist_critics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritics_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_main\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mall_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_critics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bebfe117db22>\u001b[0m in \u001b[0;36mmovies\u001b[0;34m(catalog)\u001b[0m\n\u001b[1;32m     17\u001b[0m         print('\\r2/4 — {:.2%} of movie URLs scraped. Error rate: {:.2%}'.format(ix/len(catalog),\n\u001b[1;32m     18\u001b[0m                                         errors/ix), end='   ')\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r{} movie URLs successfully scraped. Error rate: {:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmovies_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ix' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    critic_main = critics_letters(alphabet)\n",
    "    list_critics = critics_list(critic_main)\n",
    "    all_movies = movies(list_critics)\n",
    "\n",
    "    df = pd.DataFrame(all_movies)\n",
    "    df.to_csv(r'movies.csv', header=False, index=None)\n",
    "    all_movies = pd.read_csv('movies.csv', header=None)\n",
    "\n",
    "    review_pages = review_pages(all_movies)\n",
    "    pd.DataFrame(review_pages).to_csv('all_pages.csv', index=False, header=None)\n",
    "    review_pages = pd.read_csv('all_pages.csv', header=None)\n",
    "\n",
    "    rating_reviews = rating_review(review_pages)\n",
    "    pd.DataFrame(rating_reviews).to_csv('reviews.csv', index=False, header=None)\n",
    "    df = pd.read_csv('reviews.csv', header=None)\n",
    "\n",
    "    final_doc = process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
